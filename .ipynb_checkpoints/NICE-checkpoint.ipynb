{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NICE: Non-linear Independent Components Estimation\n",
    "\n",
    "We will apply this algorithm to the MNIST data set to generate new handwritten digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Hyperparameters](#1st)\n",
    "2. [Import libraries, define classes and functions](#2nd)\n",
    "3. [Data preprocessing](#3rd)\n",
    "4. [Model](#4th)\n",
    "5. [Training](#5th)\n",
    "6. [Nearest neighbour](#6th)\n",
    "7. [References](#7th)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='1st'/>\n",
    "\n",
    "## 1. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "  'MODEL_SAVE_PATH': './save/models/',\n",
    "\n",
    "  'USE_CUDA': False, #True,\n",
    "\n",
    "  'TRAIN_BATCH_SIZE': 256,\n",
    "\n",
    "  'TRAIN_EPOCHS': 2, # 75 initially!!\n",
    "\n",
    "  'NUM_COUPLING_LAYERS': 4,\n",
    "\n",
    "  'NUM_NET_LAYERS': 4,  # 6 initially # neural net layers for each coupling layer\n",
    "\n",
    "  'NUM_HIDDEN_UNITS': 500 # 1000\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='2nd'/>\n",
    "\n",
    "## 2. Import libraries, define classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Distribution, Uniform\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import save_image\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **additive coupling layer**, after deciding the partitions of the image space $I_1$ and $I_2$, basically goes as follows:\n",
    "$$y_{I_1} = x_{I_1}$$\n",
    "$$y_{I_2} = x_{I_2} + m(x_{I_1})$$\n",
    "This makes sure that the transformation is invertible. The way this is done in the class is by using masks so that we can basically choose any convenient partition we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CouplingLayer(nn.Module):\n",
    "  \"\"\"\n",
    "  Implementation of the additive coupling layer from section 3.2 of the NICE\n",
    "  paper.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, data_dim, hidden_dim, mask, num_layers=4):\n",
    "    super().__init__()\n",
    "\n",
    "    assert data_dim % 2 == 0\n",
    "\n",
    "    self.mask = mask\n",
    "\n",
    "    modules = [nn.Linear(data_dim, hidden_dim), nn.LeakyReLU(0.2)]\n",
    "    for i in range(num_layers - 2):\n",
    "      modules.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "      modules.append(nn.LeakyReLU(0.2))\n",
    "    modules.append(nn.Linear(hidden_dim, data_dim))\n",
    "\n",
    "    # the function m of the coupling layer is a neural net.\n",
    "    self.m = nn.Sequential(*modules)\n",
    "\n",
    "  def forward(self, x, logdet, invert=False):\n",
    "    if not invert:\n",
    "      x1, x2 = self.mask * x, (1. - self.mask) * x\n",
    "      y1, y2 = x1, x2 + (self.m(x1) * (1. - self.mask))\n",
    "      return y1 + y2, logdet\n",
    "\n",
    "    # Inverse additive coupling layer\n",
    "    y1, y2 = self.mask * x, (1. - self.mask) * x\n",
    "    x1, x2 = y1, y2 - (self.m(y1) * (1. - self.mask))\n",
    "    return x1 + x2, logdet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **scaling layer** is only scaling each element of the transformation by a factor:\n",
    "$$y_i = S_i x_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScalingLayer(nn.Module):\n",
    "  \"\"\"\n",
    "  Implementation of the scaling layer from section 3.3 of the NICE paper.\n",
    "  \"\"\"\n",
    "  def __init__(self, data_dim):\n",
    "    super().__init__()\n",
    "    self.log_scale_vector = nn.Parameter(torch.randn(1, data_dim, requires_grad=True))\n",
    "\n",
    "  def forward(self, x, logdet, invert=False):\n",
    "    log_det_jacobian = torch.sum(self.log_scale_vector)\n",
    "\n",
    "    if invert:\n",
    "        return torch.exp(- self.log_scale_vector) * x, logdet - log_det_jacobian\n",
    "\n",
    "    return torch.exp(self.log_scale_vector) * x, logdet + log_det_jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticDistribution(Distribution):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "  def log_prob(self, x):\n",
    "    return -(F.softplus(x) + F.softplus(-x))\n",
    "\n",
    "  def sample(self, size):\n",
    "    if cfg['USE_CUDA']:\n",
    "      z = Uniform(torch.cuda.FloatTensor([0.]), torch.cuda.FloatTensor([1.])).sample(size)\n",
    "    else:\n",
    "      z = Uniform(torch.FloatTensor([0.]), torch.FloatTensor([1.])).sample(size)\n",
    "\n",
    "    return torch.log(z) - torch.log(1. - z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='3rd'/>\n",
    "\n",
    "## 3. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing:\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "dataset = datasets.MNIST(root='./data/mnist', train=True, transform=transform, download=True)\n",
    "test = datasets.MNIST(root='./data/mnist', train=False, transform=transform, download=True)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset=dataset, batch_size=cfg['TRAIN_BATCH_SIZE'],\n",
    "                                         shuffle=True, pin_memory=True)\n",
    "testloader = torch.utils.data.DataLoader(dataset=test, batch_size=cfg['TRAIN_BATCH_SIZE'],\n",
    "                                         shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='4th'/>\n",
    "\n",
    "## 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NICE(nn.Module):\n",
    "  def __init__(self, data_dim, num_coupling_layers=3):\n",
    "    super().__init__()\n",
    "\n",
    "    self.data_dim = data_dim\n",
    "    # alternating mask orientations for consecutive coupling layers\n",
    "    # The mask will be used as the partition of each additive coupling layer...\n",
    "    # For each layer we change orientation :)\n",
    "    masks = [self._get_mask(data_dim, orientation=(i % 2 == 0))\n",
    "                                            for i in range(num_coupling_layers)]\n",
    "\n",
    "    self.coupling_layers = nn.ModuleList([CouplingLayer(data_dim=data_dim,\n",
    "                                hidden_dim=cfg['NUM_HIDDEN_UNITS'],\n",
    "                                mask=masks[i], num_layers=cfg['NUM_NET_LAYERS'])\n",
    "                              for i in range(num_coupling_layers)])\n",
    "\n",
    "    self.scaling_layer = ScalingLayer(data_dim=data_dim)\n",
    "\n",
    "    self.prior = LogisticDistribution()\n",
    "\n",
    "  def forward(self, x, invert=False):\n",
    "    if not invert:\n",
    "      z, log_det_jacobian = self.f(x)\n",
    "      log_likelihood = torch.sum(self.prior.log_prob(z), dim=1) + log_det_jacobian\n",
    "      return z, log_likelihood\n",
    "\n",
    "    return self.f_inverse(x)\n",
    "\n",
    "  def f(self, x):\n",
    "    z = x\n",
    "    log_det_jacobian = 0\n",
    "    for i, coupling_layer in enumerate(self.coupling_layers):\n",
    "      z, log_det_jacobian = coupling_layer(z, log_det_jacobian)\n",
    "    z, log_det_jacobian = self.scaling_layer(z, log_det_jacobian)\n",
    "    return z, log_det_jacobian\n",
    "\n",
    "  def f_inverse(self, z):\n",
    "    x = z\n",
    "    x, _ = self.scaling_layer(x, 0, invert=True)\n",
    "    for i, coupling_layer in reversed(list(enumerate(self.coupling_layers))):\n",
    "      x, _ = coupling_layer(x, 0, invert=True)\n",
    "    return x\n",
    "\n",
    "  def sample(self, num_samples):\n",
    "    z = self.prior.sample([num_samples, self.data_dim]).view(num_samples, self.data_dim) # self.samples before...\n",
    "    return self.f_inverse(z)\n",
    "\n",
    "  def _get_mask(self, dim, orientation=True):\n",
    "    mask = np.zeros(dim)\n",
    "    mask[::2] = 1.\n",
    "    if orientation:\n",
    "      mask = 1. - mask     # flip mask orientation\n",
    "    mask = torch.tensor(mask)\n",
    "    if cfg['USE_CUDA']:\n",
    "      mask = mask.cuda()\n",
    "    return mask.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and criterion:\n",
    "model = NICE(data_dim=784, num_coupling_layers=cfg['NUM_COUPLING_LAYERS'])\n",
    "if cfg['USE_CUDA']:\n",
    "  device = torch.device('cuda')\n",
    "  model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "state_dict = torch.load('save/models/59.pt')\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='5th'/>\n",
    "\n",
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING:\n",
    "testL = np.zeros(cfg['TRAIN_EPOCHS'])\n",
    "trainL = np.zeros(cfg['TRAIN_EPOCHS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 completed. Log Likelihood: 2038.4735107421875\n",
      "Test Log Likelihood: 1882.8900146484375\n",
      "Epoch 1 completed. Log Likelihood: 2046.1549072265625\n",
      "Test Log Likelihood: 1896.568115234375\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(cfg['TRAIN_EPOCHS']):\n",
    "  mean_likelihood = 0.0\n",
    "  num_minibatches = 0\n",
    "\n",
    "  model.train()\n",
    "\n",
    "  # TRAINING\n",
    "  for batch_id, (x, _) in enumerate(dataloader):\n",
    "      x = x.view(-1, 784) + torch.rand(784) / 256. # Uniform noise between 0 and 1/256 at most!\n",
    "      if cfg['USE_CUDA']:\n",
    "        x = x.cuda()\n",
    "\n",
    "      x = torch.clamp(x, 0, 1) # Make sure values between 0 and 1.\n",
    "\n",
    "      z, likelihood = model(x)\n",
    "      loss = -torch.mean(likelihood)   # NLL\n",
    "\n",
    "      loss.backward()\n",
    "      opt.step()\n",
    "      model.zero_grad()\n",
    "\n",
    "      mean_likelihood -= loss\n",
    "      num_minibatches += 1\n",
    "\n",
    "      #print(mean_likelihood / num_minibatches)\n",
    "      #print(batch_id)\n",
    "\n",
    "  mean_likelihood /= num_minibatches\n",
    "  trainL[epoch] = mean_likelihood\n",
    "  print('Epoch {} completed. Log Likelihood: {}'.format(epoch, mean_likelihood))\n",
    "  \n",
    "  # TEST EVALUATION\n",
    "  ml = 0\n",
    "  num_minibatches = 0\n",
    "  for batch_id, (x, _) in enumerate(testloader):\n",
    "      x = x.view(-1, 784)\n",
    "      z, likelihood = model(x)\n",
    "      l = -torch.mean(likelihood)\n",
    "\n",
    "      ml -= l\n",
    "      num_minibatches += 1\n",
    "\n",
    "  ml /= num_minibatches\n",
    "  testL[epoch] = ml\n",
    "  print('Test Log Likelihood: {}'.format(ml))\n",
    "\n",
    "  # OTHERS  \n",
    "  plt.figure()\n",
    "  pylab.xlim(0, cfg['TRAIN_EPOCHS'] + 1)\n",
    "  plt.plot(range(1, cfg['TRAIN_EPOCHS'] + 1), testL, label='test loss')\n",
    "  plt.plot(range(1, cfg['TRAIN_EPOCHS'] + 1), trainL, label='train loss')\n",
    "  plt.legend()\n",
    "  plt.savefig(os.path.join('save/values_graphs', 'loss.pdf'))\n",
    "  plt.close()\n",
    "\n",
    "  if epoch % 5 == 0:\n",
    "    save_path = os.path.join(cfg['MODEL_SAVE_PATH'], '{}.pt'.format(epoch))\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "\n",
    "    # SOME SAMPLING:\n",
    "    model.train(False)\n",
    "\n",
    "    fake_images = model.sample(20).round() # round to 0, 1\n",
    "    fake_images = fake_images.view(fake_images.size(0), 1, 28, 28)\n",
    "    sample_dir = 'samples'\n",
    "    save_image(fake_images.data, os.path.join(sample_dir, 'fake_images-{}.png'.format(epoch)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='6th'/>\n",
    "\n",
    "## 6. Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEAREST NEIGHBOURS:\n",
    "# Images 28x28, search the closest one.\n",
    "# function(generated_image) --> closest training_image\n",
    "if NN == True:\n",
    "  aux_data_loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                                batch_size=1,\n",
    "                                                shuffle=False)\n",
    "\n",
    "  def nearest_gt(generated_image):\n",
    "      min_d = 0\n",
    "      closest = False\n",
    "      for i, (image, _) in enumerate(aux_data_loader):\n",
    "          image = image.view(1, 28, 28).round() # all distances in binary...\n",
    "          d = torch.dist(generated_image,image) # must be torch tensors (1,28,28)\n",
    "          if i == 0 or d < min_d:\n",
    "              min_d = d\n",
    "              closest = image\n",
    "\n",
    "      return closest\n",
    "\n",
    "  fake_images = model.sample(24).round() # round to 0, 1\n",
    "  fake_images = fake_images.view(24, 1, 28, 28)\n",
    "  save_image(fake_images, './samples/f24.png')\n",
    "  NN = torch.zeros(24, 1, 28, 28)\n",
    "  for i in range(0,24):\n",
    "        NN[i] = nearest_gt(fake_images[i])\n",
    "        print(i)\n",
    "  save_image(NN.data, './samples/NN24.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id='7th'/>\n",
    "\n",
    "## 7. References\n",
    "\n",
    "[1] https://github.com/DakshIdnani/pytorch-nice"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
